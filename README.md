<a name="readme-top"></a>

[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![LinkedIn][linkedin-shield]][linkedin-url]

# Airflow-ETL-AdventureWorks
## 1. About This Project
This project is an ETL (Extract, Transform, Load) pipeline designed to load, transform, and store data from the AdventureWorks dataset in a data warehouse following data warehouse architecture. The pipeline integrates multiple technologies, including Apache Airflow, Spark, Hadoop, and Hive, to ensure efficient data processing, storage, and accessibility.

**Purpose**: Automate data ingestion and transformation, providing a structured data repository for business intelligence and analysis.

## 2. Technologies Used
- **Apache Airflow**: Orchestrates and schedules the ETL workflows.
- **Apache Spark**: Processes and transforms data within the Hadoop Distributed File System (HDFS).
- **Hadoop (HDFS)**: Stores data in a scalable, distributed system.
- **Apache Hive**: Acts as the data warehouse layer, storing transformed data for querying and analysis.
- **Docker**: Manages containerized environments for each component.
## 3. Project Structure
```
ðŸ“¦ Airflow-ETL-AdventureWorks
â”œâ”€Â Makefile
â”œâ”€Â README.md
â”œâ”€Â containers
â”‚Â Â â”œâ”€Â airflow
â”‚Â Â â”‚Â Â â”œâ”€Â Dockerfile
â”‚Â Â â”‚Â Â â”œâ”€Â airflow.env
â”‚Â Â â”‚Â Â â””â”€Â requirements.txt
â”‚Â Â â”œâ”€Â hadoop
â”‚Â Â â”‚Â Â â””â”€Â hadoop.env
â”‚Â Â â”œâ”€Â hive
â”‚Â Â â”‚Â Â â”œâ”€Â hdfs-site.xml
â”‚Â Â â”‚Â Â â””â”€Â hive-site.xml
â”‚Â Â â””â”€Â spark
â”‚Â Â Â Â Â â”œâ”€Â spark-master.env
â”‚Â Â Â Â Â â””â”€Â spark-worker.env
â”œâ”€Â data
â”œâ”€Â docker-compose.ym
â”œâ”€Â logs
â”‚Â Â â”œâ”€Â airflow
â”‚Â Â â””â”€Â hiveserve
â”œâ”€Â script
â”‚Â Â â”œâ”€Â create_repo_dirs.sh
â”‚Â Â â””â”€Â download_data.sh
â””â”€Â src
Â Â Â â”œâ”€Â config
Â Â Â â”‚Â Â â””â”€Â config_services.py
Â Â Â â”œâ”€Â dags
Â Â Â â”‚Â Â â”œâ”€Â etl-adventureworks.py
Â Â Â â”‚Â Â â””â”€Â hql
Â Â Â â”‚Â Â Â Â Â â””â”€Â create_hive_tbls.hql
Â Â Â â”œâ”€Â jobs
Â Â Â â”‚Â Â â”œâ”€Â load_dim_dates.py
Â Â Â â”‚Â Â â”œâ”€Â load_fct_sales.py
Â Â Â â”‚Â Â â”œâ”€Â load_stg_dim_customer.py
Â Â Â â”‚Â Â â”œâ”€Â load_stg_dim_employee.py
Â Â Â â”‚Â Â â”œâ”€Â load_stg_dim_geography.py
Â Â Â â”‚Â Â â”œâ”€Â load_stg_dim_product.py
Â Â Â â”‚Â Â â”œâ”€Â load_stg_dim_promotion.py
Â Â Â â”‚Â Â â”œâ”€Â load_stg_dim_salesterritory.py
Â Â Â â”‚Â Â â””â”€Â load_stg_sales.py
Â Â Â â””â”€Â utils
Â Â Â Â Â Â â””â”€Â utils.py
```
Â©generated by [Project Tree Generator](https://woochanleee.github.io/project-tree-generator)
## 4. About the Pipeline
This ETL pipeline loads local data into Hadoop, processes and transforms it with Spark, and stores it in Hive following data warehouse architecture. The pipeline includes:

### Data Flow: Structured data flow from raw input to refined warehouse tables.
### Airflow DAG: Orchestrates tasks such as data loading, transformation, and loading into the data warehouse.
Data Warehouse Schema: Organized into dimension and fact tables, storing sales, customer, employee, geography, product, promotion, and sales territory data.
# Installation
## 1. Setup
### Clone the project:
```
git clone https://github.com/Doanh-Chinh/Airflow-ETL-AdventureWorks
cd Airflow-ETL-AdventureWorks
```
### Build and start the containers:
```
make setup
```
### Tear Down
To stop and remove all containers:
```
make down
```
## 2. Getting Started
### Interfaces
Access each serviceâ€™s UI:

- `Airflow`: http://localhost:8080
- `Spark`: http://localhost:8081
- `HDFS`: http://localhost:9870
- `Hive`: Through the Hive CLI in the container
### Query Pipeline Output in Data Warehouse
Run queries in Hive CLI within the container to access processed data in the data warehouse tables.


<!-- Badges -->
[forks-shield]: https://img.shields.io/github/forks/Doanh-Chinh/Airflow-ETL-AdventureWorks.svg?style=for-the-badge
[forks-url]: https://github.com/Doanh-Chinh/Airflow-ETL-AdventureWorks/network/members
[stars-shield]: https://img.shields.io/github/stars/Doanh-Chinh/Airflow-ETL-AdventureWorks.svg?style=for-the-badge
[stars-url]: https://github.com/Doanh-Chinh/Airflow-ETL-AdventureWorks/stargazers
[linkedin-shield]: https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin
[linkedin-url]: https://www.linkedin.com/in/chinh-luong-doanh/
