# Airflow config (based on Airflow docs)
# (https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#)
AIRFLOW_HOME=/opt/airflow
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=False

# Airflow connections
AIRFLOW_CONN_HDFS_DEFAULT=hdfs://namenode:8020
AIRFLOW_CONN_WEBHDFS_DEFAULT=webhdfs://namenode:9870
AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
AIRFLOW_CONN_HIVE_CLI_DEFAULT=hive-cli://hive-server:10000/?use_beeline=True

# Airflow staging data
STG_DIM_PROMOTION=/stages/stg_dim_promotion.parquet
STG_DIM_CUSTOMER=/stages/stg_dim_customer.parquet
STG_DIM_EMPLOYEE=/stages/stg_dim_employee.parquet
STG_DIM_GEOGRAPHY=/stages/stg_dim_geography.parquet
STG_DIM_SALES_TERRITORY=/stages/stg_dim_sales_territory.parquet
STG_DIM_PRODUCT=/stages/stg_dim_product.parquet
STG_FACT_SALES=/stages/stg_fact_sales.parquet
INFERRED_MEMBER_PATH=/inferred_member/inferred_member.parquet

# Airflow variables
AIRFLOW_VAR_WEBHDFS_HOST=host.docker.internal
AIRFLOW_VAR_WEBHDFS_PORT=9870


# For spark-submit
JAVA_HOME="/usr/lib/jvm/java-17-openjdk-amd64"

# Python environments
PYTHONDONTWRITEBYTECODE=1
PYTHONUNBUFFERED=1